#!/usr/bin/python
# coding=utf-8
#
# Copyright (C) 2012 Allis Tauri <allista@gmail.com>
# 
# indicator_gddccontrol is free software: you can redistribute it and/or modify it
# under the terms of the GNU General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
# 
# indicator_gddccontrol is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
# See the GNU General Public License for more details.
# 
# You should have received a copy of the GNU General Public License along
# with this program.  If not, see <http://www.gnu.org/licenses/>.
'''
Created on Oct 14, 2014

@author: Allis Tauri <allista@gmail.com>
'''

try:
    from Bio import SeqIO
    from Bio import Entrez
except ImportError:
    print'BioPython must be installed in your system.'
    raise 

import os
import sys
import traceback
import logging
import argparse
from time import time, sleep
from datetime import timedelta
from collections import Counter
import csv

#------------------------------------------------------------------------------#
class OutIntercepter(object):
    '''A file-like object which intercepts std-out/err'''
    def __init__(self):
        self._oldout = None
        self._olderr = None
    #end def
    
    def write(self, text): pass
    
    def flush(self): pass
    
    def __enter__(self):
        self._oldout = sys.stdout
        self._olderr = sys.stderr
        sys.stdout = sys.stdout = self
        return self
    #end def
    
    def __exit__(self, _type, _value, _traceback):
        if _type is not None and not SystemExit:
            print(_value)
            traceback.print_exception(_type, _value, _traceback, file=self._olderr)
        sys.stdout = self._oldout
        sys.stderr = self._olderr
        return True
    #end def
#end class


class EchoLogger(OutIntercepter):
    '''Wrapper around logging module to capture stdout-err into a log file
    while still print it to std'''

    def __init__(self, name, level=logging.INFO):
        OutIntercepter.__init__(self)
        self._name      = name
        self._log       = name+'.log'
        self._level     = level
        self._logger    = logging.getLogger(name)
        self._handler   = logging.FileHandler(self._log, encoding='UTF-8')
        self._formatter = logging.Formatter('[%(asctime)s] %(message)s')
        #set log level
        self._handler.setLevel(self._level)
        self._logger.setLevel(self._level)
        #assemble pipeline
        self._handler.setFormatter(self._formatter)
        self._logger.addHandler(self._handler)
    #end def
        
    def __del__(self):
        self._handler.close()
        
        
    def __enter__(self):
        OutIntercepter.__enter__(self)
        self._logger.log(self._level, '=== START LOGGING ===')
        return self
    #end def
    
    def __exit__(self, _type, _value, _traceback):
        if _type is not None and not SystemExit:
            print(_value)
            self._logger.error('Exception occured:', 
                               exc_info=(_type, _value, _traceback))
            traceback.print_exception(_type, _value, _traceback, file=self._olderr)
        sys.stdout = self._oldout
        sys.stderr = self._olderr
        self._logger.log(self._level, '=== END LOGGING ===')
        return True
    #end def
        
    def write(self, text):
        log_text = ' '.join(text.split())
        if log_text: self._logger.log(self._level, log_text, exc_info=False)
        self._oldout.write(text)
    #end def
    
    def flush(self): self._oldout.flush()
#end class


def load_sequences(filename):
    '''generate a SeqRecord object with sequence from a raw string or a file'''
    #check if filename is an existing file
    records = []
    if os.path.isfile(filename):
        #parse file
        try:
            records = list(SeqIO.parse(filename, 'fasta'))
        except Exception as e:
            print(e._message)
            raise ValueError('load_sequence: unable to parse %s.' % filename)
    else: print('No such file: "%s"' % filename)
    return records
#end def


def get_isolation_sources(rec):
    for f in rec.features:
        if f.type != 'source': continue
        try:
            return f.qualifiers['isolation_source']
        except KeyError:
            print('%s: the "source" feature does not contain "isolation_source" qualifier\n' % r.id)
            return ['isolation source is not provided']
#end def


def retry(func, error_msg):
    for i in xrange(_NUM_RETRIES):
        try:
            result = func()
            break
        except Exception as e:
            print(e)
            if i == _NUM_RETRIES-1:
                raise RuntimeError(error_msg)
    return result
#end def
        

def get_records_for_ids(records, start=0, num=20):
    #compose a query
    subset = records[start:start+num]
    stop   = start+len(subset)
    query  = ' or '.join(rec.id.split('.')[0] for rec in subset)
    #perform the query
    print('Query with accession numbers %d-%d' % (start+1, stop))
    results = retry(lambda : Entrez.read(Entrez.esearch(db='nucleotide', term=query, usehistory="y")),
                    'Unable to get Entrez IDs for sequences %d-%d' % (start+1, stop))
    if not results['IdList']:
        print('NCBI returned no result for the accession numbers from sequences %d-%d' 
              % (start+1, stop))
        return []
    webenv    = results['WebEnv']
    query_key = results['QueryKey']
    #fetch genbank data for the received IDs 
    num_results = len(results['IdList'])
    print('Downloading data...')
    data = retry(lambda : Entrez.efetch(db="nucleotide", rettype="gb", retmode="text",
                                    retstart=0, retmax=num_results, complexity=4,
                                    webenv=webenv, query_key=query_key),
                 'Unable to download data for sequences %d-%d' % (start+1, stop))
    #parse received data
    try: records = list(SeqIO.parse(data, 'gb'))
    except Exception as e: 
        print(e.message)
        return []
    finally: data.close()
    print('Done. Elapsed time: %s\n' % timedelta(seconds=time()-_START_TIME))
    return records
#end def
#------------------------------------------------------------------------------#

_NUM_RETRIES = 3
_PAUSE_EACH  = 100
_BATCH       = 20
_START_TIME  = time()

#------------------------------------------------------------------------------#
if __name__ == '__main__':
    #parse command line arguments
    parser = argparse.ArgumentParser(description='Gets isolation sources given '
                                     'the set of sequences in fasta format with '
                                     'specified accession' 
                                     'numbers as given by SILVA tools.')
    parser.add_argument('-e --email', dest='email', metavar='address@domain.com', type=str, nargs=1, required=True,
                        help='Your e-mail is required by NCBI when you use Entrez API.')
    parser.add_argument('-p --pause', dest='pause', metavar='sec', type=int, nargs=1, default=60,
                        help='Pause in seconds in between of series of Entrez queries.')
    parser.add_argument('-r --retries', dest='retries', metavar='times', type=int, nargs=1, default=3,
                        help='Number of retries on network failure.')
    parser.add_argument('files', metavar='file', type=str, nargs='+',
                        help='File(s) in fasta format with sequences and accession numbers.')
    args = parser.parse_args()
    _NUM_RETRIES = args.retries;
    #job name and working dir
    jobname = os.path.basename(args.files[0]).split('.')[0]
    dirname = os.path.dirname(args.files[0])
    with EchoLogger(os.path.join(dirname, jobname)):
        #load sequences
        seq_records = []
        for filename in args.files:
            try: seq_records.extend(load_sequences(filename))
            except ValueError: continue
        #check number of queries and warn the user
        num_recs = len(seq_records)
        num_queries = num_recs/_BATCH
        num_pauses = 0; pause_time = 0 
        if num_queries > _PAUSE_EACH:
            num_pauses = num_queries/_PAUSE_EACH
            pause_time = num_pauses * args.pause
            _PAUSE_EACH = num_queries/(num_pauses+1)+1
            print('WARNING: %d separate Entrez queries will be made.\n'
                  'To comply with NCBI rules the queries will be made\n'
                  'in series of %d with %d sec pause in between.\n' 
                  % (num_queries, _PAUSE_EACH, args.pause))
            print('Total pause time will be:\n%s\n' % timedelta(seconds=pause_time))
        query_time = num_queries * 1/3.0
        if query_time > 5:
            print('No more than 3 requests per second is allowed by NCBI,\n'
                  'so *minimum* time spend for your query will be:\n%s\n' % timedelta(seconds=query_time))
        if pause_time > 0 and query_time > 5:
            print('Total *minimum* estimated time:\n%s\n' % timedelta(seconds=pause_time+query_time))
            print('Note, that depending on the load of NCBI servers it\n'
                  'may take several times as much.\n')
        #setup Entrez engine
        Entrez.email = args.email
        Entrez.tool = 'GetIsolationSources'
        #perform queries in batches and write results to a csv file
        unique_sources = Counter()
        all_sources_filename = os.path.join(dirname, jobname+'-sources.csv')
        with open(all_sources_filename, 'wb') as csvfile:
            output = csv.writer(csvfile)
            pause_num = _PAUSE_EACH
            for i in xrange(0, num_recs, _BATCH):
                if i/_BATCH > pause_num:
                    print('Pausing for %d seconds...\n' % args.pause)
                    sleep(args.pause)
                    pause_num += _PAUSE_EACH
                try: records = get_records_for_ids(seq_records, i, _BATCH)
                except RuntimeError as e: 
                    print(e.message)
                    print('Query aborted.')
                    sys.exit(1)
                for r in records: 
                    sources = get_isolation_sources(r)
                    if sources: 
                        output.writerow([r.id]+sources)
                        for s in sources: unique_sources[s.lower()] += 1
        #write a histogramm of isolation sources in a csv file
        unique_sources_filename = os.path.join(dirname, jobname+'-source-distribution.csv')
        print('All isolation sources with corresponding GenBank IDs were written to:\n   %s\n'
              'Writing the source distribution histogram to:\n   %s\n' 
              % (all_sources_filename, unique_sources_filename))
        with open(unique_sources_filename, 'wb') as csvfile:
            output = csv.writer(csvfile)
            map(lambda s: output.writerow(s), unique_sources.most_common())
        print('Done.\nTotal elapsed time: %s\n' % timedelta(seconds=time()-_START_TIME))
#end